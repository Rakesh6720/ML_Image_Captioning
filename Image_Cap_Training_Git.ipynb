{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision Nanodegree\n",
    "\n",
    "## Project: Image Captioning\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, I demonstrate the training of my CNN-RNN model.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step1'></a>\n",
    "## Step 1: Training Setup\n",
    "\n",
    "In this step of the notebook, you I customize the training of my CNN-RNN model by specifying hyperparameters and setting other options that are important to the training procedure.\n",
    "\n",
    "\n",
    "### A Description of the Training Variables (initialized in the next cell):\n",
    "\n",
    "- `batch_size` - the batch size of each training batch.  It is the number of image-caption pairs used to amend the model weights in each training step. \n",
    "- `vocab_threshold` - the minimum word count threshold.  Note that a larger threshold will result in a smaller vocabulary, whereas a smaller threshold will include rarer words and result in a larger vocabulary.  \n",
    "- `vocab_from_file` - a Boolean that decides whether to load the vocabulary from file. \n",
    "- `embed_size` - the dimensionality of the image and word embeddings.  \n",
    "- `hidden_size` - the number of features in the hidden state of the RNN decoder.  \n",
    "- `num_epochs` - the number of epochs to train the model.  We recommend that you set `num_epochs=3`, but feel free to increase or decrease this number as you wish.  [This paper](https://arxiv.org/pdf/1502.03044.pdf) trained a captioning model on a single state-of-the-art GPU for 3 days, but you'll soon see that you can get reasonable results in a matter of a few hours!  (_But of course, if you want your model to compete with current research, you will have to train for much longer._)\n",
    "- `save_every` - determines how often to save the model weights.  We recommend that you set `save_every=1`, to save the model weights after each epoch.  This way, after the `i`th epoch, the encoder and decoder weights will be saved in the `models/` folder as `encoder-i.pkl` and `decoder-i.pkl`, respectively.\n",
    "- `print_every` - determines how often to print the batch loss to the Jupyter notebook while training.  Note that you **will not** observe a monotonic decrease in the loss function while training - this is perfectly fine and completely expected!  You are encouraged to keep this at its default value of `100` to avoid clogging the notebook, but feel free to change it.\n",
    "- `log_file` - the name of the text file containing - for every step - how the loss and perplexity evolved during training.\n",
    "\n",
    "### Question Prompts from the Udacity Instructor:\n",
    "\n",
    "### Question 1\n",
    "\n",
    "**Question:** Describe your CNN-RNN architecture in detail.  With this architecture in mind, how did you select the values of the variables in Task 1?  If you consulted a research paper detailing a successful implementation of an image captioning model, please provide the reference.\n",
    "\n",
    "**My Answer:** \n",
    "As a model for my CNN I used the architecture of the Resnet50 network.  The  architecture of the Resnet model can be found <a href=\"https://cv-tricks.com/keras/understand-implement-resnets/\">here</a>.\n",
    "\n",
    "For my RNN decoder I created a model that has: an embedding layer, 2 layers of an LSTM with 512 dimensions, and a fully connected layer that creates a tensor of output scores for my vocabulary.  \n",
    "\n",
    "For my mini-batch size I chose the value 256 because the video lecture in the course suggested a larger batch size could help reduce the error of a relatively large learning rate.  While training my model the first time I noticed my error rate increasing.  I stopped the training and decreased the learning rate.  However, because I did not want to spend too much of my GPU time training I did not reduce the learning rate by much, which would have increased my computation time.  I chose to compromise by lowering the learning rate a bit but increasing my batch size from 32 to 256.\n",
    "\n",
    "For my vocab_threshold value I chose 5 because that was the value used by the researchers in the research paper \"Show and Tell: a Neural Image Caption Generator.\"  \n",
    "\n",
    "For the embed_size, I chose 256 because that was the value suggested by the lectures in the course.  The course lecture also suggested that the size of the LSTM's hidden layer should be larger than the size of the embedding layer.  The \"Show and Tell\" paper suggested a hidden layer size of 512, which I also used.\n",
    "\n",
    "I chose to train my model for 3 epochs because after trial and error it became apparent to me that 3 epochs was all the time I could judiciously alot for the project.  Since the error rate fell continuously throughout I felt no need to prematurely end the traning before the end of its third epoch.\n",
    "\n",
    "I set the File Load value to True because I had already run the cell that created the vocabulary in a previous notebook.  \n",
    "\n",
    "Finally, I set the print parameter to 250 because from trial-and-error I did not find it useful to have a message print more often than that.\n",
    "\n",
    "\n",
    "### Question 2\n",
    "\n",
    "**Question:** How did you select the transform in `transform_train`?  If you left the transform at its provided value, why do you think that it is a good choice for your CNN architecture?\n",
    "\n",
    "**My Answer:** \n",
    "I left the parameters of the transform Udacity selected as they were because the settings seemed appropriate for the architecture I chose for my model.  For example, the image size of 224 x 224 matched the requirements of the Resnet CNN and the 0.5 value probability of randomly flipping each image made sense to me for the purposes of data augmentation.  Lastly, I kept the mean and standard deviation values as they were for the Transform's Normalize method because I did not have a reason to imagine I could come up with anything more appropriate.  \n",
    "\n",
    "### Question 3\n",
    "\n",
    "**Question:** How did you select the trainable parameters of your architecture?  Why do you think this is a good choice?\n",
    "\n",
    "**My Answer:** \n",
    "I chose to train all the weights in my decoder and only the fully connected layer of my CNN encoder.  I chose to leave the other weights in the CNN as they were because the model had already been trained on a robust dataset.  By training only the weights on the fully connected layer I could fine-tune the Resnet CNN model to the specific images of my dataset.  In my opinion, this choice would strike a balance between accuracy and overfitting.  \n",
    "\n",
    "I chose to train all the weights of my decoder because I was not beginning with a pre-trained model.  \n",
    "\n",
    "### Question 4\n",
    "\n",
    "**Question:** How did you select the optimizer used to train your model?\n",
    "\n",
    "**My Answer:** \n",
    "I chose to use the Adam Optimizer because in the course lecture on hyperparameters the instructor suggested the Adam Optimizer implemented a learning rate decay through adaptive learning.  Because I was not certain about the balance I struck between my batch size value and my learning rate value in my hyperparameters, I felt comfortable relying a bit on the Adam Optimizer's learning rate adjustment to help guide my model toward a reasonable design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary successfully loaded from vocab.pkl file!\n",
      "loading annotations into memory...\n",
      "Done (t=0.91s)\n",
      "creating index...\n",
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414113/414113 [01:36<00:00, 4300.88it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import sys\n",
    "sys.path.append('/opt/cocoapi/PythonAPI')\n",
    "from pycocotools.coco import COCO\n",
    "from data_loader import get_loader\n",
    "from model import EncoderCNN, DecoderRNN\n",
    "import math\n",
    "\n",
    "\n",
    "## TODO #1: Select appropriate values for the Python variables below.\n",
    "batch_size = 256          # batch size\n",
    "vocab_threshold = 5        # minimum word count threshold\n",
    "vocab_from_file = True    # if True, load existing vocab file\n",
    "embed_size = 256           # dimensionality of image and word embeddings\n",
    "hidden_size = 512          # number of features in hidden state of the RNN decoder\n",
    "num_epochs = 3             # number of training epochs\n",
    "save_every = 1             # determines frequency of saving model weights\n",
    "print_every = 250          # determines window for printing average loss\n",
    "log_file = 'training_log.txt'       # name of file with saved training loss and perplexity\n",
    "\n",
    "# (Optional) TODO #2: Amend the image transform below.\n",
    "transform_train = transforms.Compose([ \n",
    "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
    "    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "# Build data loader.\n",
    "data_loader = get_loader(transform=transform_train,\n",
    "                         mode='train',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=vocab_from_file)\n",
    "\n",
    "# The size of the vocabulary.\n",
    "vocab_size = len(data_loader.dataset.vocab)\n",
    "\n",
    "# Initialize the encoder and decoder. \n",
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "# Move models to GPU if CUDA is available. \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "# Define the loss function. \n",
    "criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    "\n",
    "# TODO #3: Specify the learnable parameters of the model.\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters())\n",
    "\n",
    "# TODO #4: Define the optimizer.\n",
    "optimizer = torch.optim.Adam(params, lr=0.0001)\n",
    "\n",
    "# Set the total number of training steps per epoch.\n",
    "total_step = math.ceil(len(data_loader.dataset.caption_lengths) / data_loader.batch_sampler.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Happening in the Cell Above?\n",
    "\n",
    "I can best explain the actions performed in the cell above by breaking its behavior down into 8 steps.\n",
    "\n",
    "<u>Step 1</u>: I import the libraries required by the functions called in the cell.  The Data_Loader and Model libraries are classes I created the files for which I have included in this repository.  \n",
    "\n",
    "<u>Step 2</u>: I set the values for the variables Udacity defined in the training setup.  I determined values for batch_size, vocab_threshold, embed_size, and hidden_size based on trial-and-error informed by the literature provided by Udacity in its lecture modules.\n",
    "\n",
    "<u>Step 3</u>: I defined a Training Transform for the training dataset using the Transform class from the PyTorch library.  \n",
    "\n",
    "<u>Step 4</u>: I set the parameters for the Data Loader object and call its constructor \"get_loader\" which I define in the Data_Loader python file included in the repository.  In addition to the variables defined in Step 2, the \"get_loader\" function takes as a parameter the Training Transform defined in Step 3.  The Training Transform serves as the final state of the images and captions in the training data set before the Data Loader object passes them to the Model for training. \n",
    "\n",
    "<u>Step 5</u>: I capture the size of the vocabulary represented by the dataset's captions and store it in a object called \"vocab_size\".  The \"vocab_size\" object accesses the \"vocab\" field built into the CoCoDataset.  Coco, in turn, calls the Vocabulary constructor, which I have included in the <strong>vocabulary.py</strong> file in the repository for this project. Among other parameters, the Vocabulary constructor uses the input I defined as the integer 5 in Step 2 as <em>\"vocab_threshold\"</em> to create a vocabulary from all words that appear in the dataset's caption set more than 5 times.\n",
    "\n",
    "<u>Step 6</u>: I initialize and store the Encoder and Decoder neural networks from the <strong>model.py</strong> file included in the repository.\n",
    "\n",
    "<u>Step 7</u>: I prepare the Encoder and Decoder neural networks to train on the GPU.\n",
    "\n",
    "<u>Step 8</u>: I define the Loss and Optimizer functions to tune my networks' accuracy, and define the learnable (updatable) parameters of my neural networks as all the parameters of the RNN Decoder and only the final embed layer of the Encoder CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step2'></a>\n",
    "## Step 2: Training My Model\n",
    "\n",
    "\n",
    "### A Note from Udacity on Training Time:\n",
    "\n",
    "For this project, you need not worry about overfitting. **This project does not have strict requirements regarding the performance of your model**, and you just need to demonstrate that your model has learned **_something_** when you generate captions on the test data.  For now, we strongly encourage you to train your model for the suggested 3 epochs without worrying about performance; then, you should immediately transition to the next notebook in the sequence to see how your model performs on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [250/1618], Loss: 4.7381, Perplexity: 114.2201\n",
      "Epoch [1/3], Step [500/1618], Loss: 4.3030, Perplexity: 73.92104\n",
      "Epoch [1/3], Step [750/1618], Loss: 4.0651, Perplexity: 58.27357\n",
      "Epoch [1/3], Step [1000/1618], Loss: 3.8360, Perplexity: 46.3381\n",
      "Epoch [1/3], Step [1250/1618], Loss: 3.7303, Perplexity: 41.69293\n",
      "Epoch [1/3], Step [1500/1618], Loss: 3.9844, Perplexity: 53.75150\n",
      "Epoch [2/3], Step [250/1618], Loss: 3.6035, Perplexity: 36.72717\n",
      "Epoch [2/3], Step [500/1618], Loss: 3.3509, Perplexity: 28.5270\n",
      "Epoch [2/3], Step [750/1618], Loss: 3.7475, Perplexity: 42.4135\n",
      "Epoch [2/3], Step [1000/1618], Loss: 3.1696, Perplexity: 23.7969\n",
      "Epoch [2/3], Step [1250/1618], Loss: 3.0535, Perplexity: 21.1900\n",
      "Epoch [2/3], Step [1500/1618], Loss: 3.7397, Perplexity: 42.0852\n",
      "Epoch [3/3], Step [250/1618], Loss: 3.1190, Perplexity: 22.62334\n",
      "Epoch [3/3], Step [500/1618], Loss: 2.8316, Perplexity: 16.9732\n",
      "Epoch [3/3], Step [750/1618], Loss: 3.2204, Perplexity: 25.03803\n",
      "Epoch [3/3], Step [1000/1618], Loss: 2.5796, Perplexity: 13.1925\n",
      "Epoch [3/3], Step [1250/1618], Loss: 2.8530, Perplexity: 17.3398\n",
      "Epoch [3/3], Step [1500/1618], Loss: 2.6101, Perplexity: 13.6001\n",
      "Epoch [3/3], Step [1618/1618], Loss: 2.5856, Perplexity: 13.2718"
     ]
    }
   ],
   "source": [
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Open the training log file.\n",
    "f = open(log_file, 'w')\n",
    "\n",
    "# --START-- Udacity-specific code to maintain GPU-connection during training\n",
    "\n",
    "old_time = time.time()\n",
    "response = requests.request(\"GET\", \n",
    "                            \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/keep_alive_token\", \n",
    "                            headers={\"Metadata-Flavor\":\"Google\"})\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    \n",
    "    for i_step in range(1, total_step+1):\n",
    "        \n",
    "\n",
    "        if time.time() - old_time > 60:\n",
    "            old_time = time.time()\n",
    "            requests.request(\"POST\", \n",
    "                             \"https://nebula.udacity.com/api/v1/remote/keep-alive\", \n",
    "                             headers={'Authorization': \"STAR \" + response.text})\n",
    "            \n",
    "# --END-- Udacity-specific code to maintain GPU-connection\n",
    "        \n",
    "        # Randomly sample a caption length, and sample indices with that length.\n",
    "        indices = data_loader.dataset.get_train_indices()\n",
    "        \n",
    "        # Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
    "        new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "        data_loader.batch_sampler.sampler = new_sampler\n",
    "        \n",
    "        # Obtain the batch.\n",
    "        images, captions = next(iter(data_loader))\n",
    "\n",
    "        # Move batch of images and captions to GPU if CUDA is available.\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        \n",
    "        # Zero the gradients.\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "        \n",
    "        # Pass the inputs through the CNN-RNN model.\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions)\n",
    "        \n",
    "        # Calculate the batch loss.\n",
    "        loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "        \n",
    "        # Backward pass.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the parameters in the optimizer.\n",
    "        optimizer.step()\n",
    "            \n",
    "        # Get training statistics.\n",
    "        stats = 'Epoch [%d/%d], Step [%d/%d], Loss: %.4f, Perplexity: %5.4f' % (epoch, num_epochs, i_step, total_step, loss.item(), np.exp(loss.item()))\n",
    "        \n",
    "        # Print training statistics (on same line).\n",
    "        print('\\r' + stats, end=\"\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        # Print training statistics to file.\n",
    "        f.write(stats + '\\n')\n",
    "        f.flush()\n",
    "        \n",
    "        # Print training statistics (on different line).\n",
    "        if i_step % print_every == 0:\n",
    "            print('\\r' + stats)\n",
    "            \n",
    "    # Save the weights.\n",
    "    if epoch % save_every == 0:\n",
    "        torch.save(decoder.state_dict(), os.path.join('./models', 'decoder-%d.pkl' % epoch))\n",
    "        torch.save(encoder.state_dict(), os.path.join('./models', 'encoder-%d.pkl' % epoch))\n",
    "\n",
    "# Close the training log file.\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results:\n",
    "After only 3 training Epochs, the error rate of my model, calculated as Loss, has decreased by more than 45%.  While a Loss value of 2.5 is significantly too high for an adequate machine learning model, the evidence that my model in fact learns is enough to fulfill the criteria of the assignment.  In order to compare the results of my model with others publicly available would require me to train the model for several hours on a GPU, the resources for which I currently do not have.  \n",
    "\n",
    "In the <strong>next notebook</strong> I will test the accuracy of my CNN-RNN model by tasking it with creating its own captions for unknown images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
